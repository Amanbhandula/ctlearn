
################

Run Name: testNet[r1]

Start time: 01/08/16 2:23
End Time: 01/08/16 16:53  

Duration: approx. 14.5 hours

Epochs: 16 completed, 

Time/epoch: ~55 min/epoch

Model: testNet

Dataset: small set - Gamma + Proton (~120000 training images)

122976 Training images
103780 gamma
19196 proton

30742 Validation images
25944 gamma
4798 proton

Hyperparameters:

Optimizer:
Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)

Learning rate scheduler:
ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)

Notes/Next Steps:

Learning seems to be stalled at around 0.843 and 2.516 training accuracy and
loss respectively. Seems consistent with labeling all events as gamma.


####################

Run Name: testNet[r2]

Start time: 01/09/16 2:23
End Time: 01/09/16 23:51  

Duration: approx. 21.5 hours

Epochs: 21

Time/epoch: ~60 min/epoch

Model: testNet

Dataset: small set - Gamma + Proton (~120000 training images)

122976 Training images
103780 gamma
19196 proton

30742 Validation images
25944 gamma
4798 proton

Hyperparameters:

Optimizer:
Nadam(lr=0.00000002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)

Learning rate scheduler:
ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)

Notes/Next Steps:

Learning rate reduced drastically from previous run.

Plateau in train loss was lowered drastically to about 0.480 but with little
apparent improvement in training accuracy. 

Observed further slow declines in training loss but no apparent improvement in validation loss or accuracy.

#############################


Run Name: VGG16[r1]

Start time: 01/10/16 00:51
End Time: 01/10/16 12:09 

Duration: ~11 hrs

Epochs: 6

Time/epoch: ~110 min/epoch

Model: VGG16 (from VGG16.py)

Dataset: small set - Gamma + Proton (~120000 training images)

122976 Training images
103780 gamma
19196 proton

30742 Validation images
25944 gamma
4798 proton

Hyperparameters:

Optimizer:
Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)

Learning rate scheduler:
ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)

Notes/Next Steps:

Used pretrained imageNet weights.

Used rgb color mode (3 channels). Shouldn't affect training ability although could cause greater GPU usage. Note possible effects.

Learning seems to have stalled at exactly 2.5162532354220106 training loss.
Not clear exactly why this occurred. Possibly due to learning rate being
reduced to 0 due to combination of nadam and learning rate reducer? Try
removing learning rate scheduler. Possibly reduce learning rate also?

#############################

Run Name: ResNet50[r1]

Start time: 01/13/16 02:40
End Time: 01/13/16 19:12 

Duration: ~16.5 hrs

Epochs: 5

Time/epoch: ~200 min/epoch

Model: ResNet50 (from ResNet50.py)

Dataset: small set - Gamma + Proton (~120000 training images)

122976 Training images
103780 gamma
19196 proton

30742 Validation images
25944 gamma
4798 proton

Hyperparameters:

Optimizer:
Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)

Notes/Next Steps:

Used pretrained imageNet weights. Learning stalled at an exact value for
training loss (again). Try using random weight initialization?

#############################
