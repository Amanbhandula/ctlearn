[Data]
# String identifying the format of the data files being loaded from. 
# Determines the subclass of DataLoader which should be used when reading.
format = option('HDF5', default='HDF5')
# String path to a text file containing a list of data files (1 per line).
file_list = string(default=None)
# Boolean indicating whether or not to apply processing to loaded data.
# Determines whether a DataProcessor is instantiated. If false,
# settings under the Data Processing are not used.
apply_processing = boolean(default=True)

# These options are used directly when instantiating a
# DataLoader.
[[Data Loading]]
# Required string. Whether the examples to be loaded should be single telescope
# (images from a single event on a single telescope). Note: this should agree with
# the model type and other settings.
example_type = option('single_tel', 'array', default='array')
# String identifying which telescope type to load.
selected_tel_type = string(default='MSTS')
# Optional list of telescope ids (integers) identifying which telescopes to load.
# Telescope ids must match the selected tel type.
selected_tel_ids = int_list(default=None)
# An integer indicating the minimum
# number of triggered images which must be 
# present in each event for it to be used.
min_num_tels = integer(min=1, default=1)
# A PyTables cut condition used when selecting events. 
# See https://www.pytables.org/usersguide/condition_syntax.html
# for explanation of syntax. The names of valid event attributes
# should be found in the HDF5 data files.
cut_condition = string(default="")
# Fraction of the data to set aside for periodic validation. 
validation_split = float(min=0.0, max=1.0, default=0.1)
# Random seed used when splitting the data into training and
# validation sets.
seed = integer(default=None)

[[Data Processing]]
# Whether or not to crop telescope images.
crop = boolean(default=True)
# Type of image cleaning to apply when calculating the weighted
# centroid position of each telescope image for cropping.
image_cleaning = option('twolevel', None, default='twolevel') 
# Whether to return the cleaned images after cropping, or only use the
# cleaning for determining the bounding box location and return the
# original images.
return_cleaned_images = boolean(default=False)
# Normalization method to be applied pixel-wise to each image. Log normalization
# shifts the pixel values to [1,infinity) by subtracting the minimum pixel value
# across all images, then takes the natural log.
normalization = option('log', None, default=None)
# Whether to order the images within each telescope type before stacking them.
# Trigger orders them so all triggered images are at the front with blank images/padding
# behind. Size sorts images by the sum of pixel values, from largest to smallest.
sort_images_by = option('trigger', 'size', None, default='trigger')

# The side length of the square region to crop telescope images
# to. Set separately for each telescope type.
[[[bounding_box_sizes]]]
__many__ = integer(min=1, default=48)

# Cleaning thresholds used for image cleaning. For two-level
# cleaning, the thresholds are a 2-tuple. 
# The first value is the picture threshold, which passes all pixels
# in the image greater than it, and the second is the boundary threshold,
# which adds all pixels greater than it and adjacent to previously selected pixels.
[[[thresholds]]]
__many__ = float_list(default=list(5.5,1.0))

# Settings used by the TF Dataset when providing data.
[[Data Input]]
# Number of examples per batch for training/prediction.
batch_size = integer(min=1, default=64)
# Whether to prefetch examples in the TF Dataset.
prefetch = boolean(default=True)
# Max number of examples to prefetch and store in buffer when
# loading data.
prefetch_buffer_size = integer(min=1, default=10)
# Whether to apply Dataset.map on examples (must be true for HDF5 data).
map = boolean(default=False)
# Number of parallel threads to use when loading examples with
# Dataset.map
num_parallel_calls = integer(min=1, default=1)
# Whether to shuffle examples.
shuffle = boolean(default=True)
# Number of examples to shuffle at once. Larger buffer sizes
# give a more uniform shuffle at the cost of a longer time to shuffle.
shuffle_buffer_size = integer(min=1, default=10000)

# Settings applied when mapping from trace to 2D images.
[Image Mapping]

[Model]
# String filepath to directory containing models.
model_directory = string(default='../models/')
# Module in ModelDirectory containing function implementing
# model. Included, verified modules are "single_tel", "cnn_rnn".
model_module = string(default='cnn_rnn')
# Function in ModelModule implementing model. Included,
# verified models are "single_tel_model" within the "single_tel" module and
# "cnn_rnn_model" within the "cnn_rnn" module.
model_function = string(default='cnn_rnn_model')

# Additional user-determined custom  model parameters
[[Model Parameters]]


[Training]
# Number of epochs to run training and validation. If 0, run
# forever.
num_epochs = integer(min=0, default=0)
# Number of training steps to run before evaluating on the 
# training and validation sets.
num_training_steps_per_validation = integer(min=1, default=1000)

[[Hyperparameters]]
# Choice of optimizer function for training.
optimizer = option('Adam', 'SGD', 'RMSProp', 'AdaDelta', default='Adam')
# Base learning rate before scaling or annealing.
base_learning_rate = float(default=0.001)
# Whether to scale the learning rate by number of triggered telescopes (for trigger dropout).
# Not used for single telescope models.
scale_learning_rate = boolean(default=False)
# Whether to weight the loss to compensate for the class
# balance.
apply_class_weights = boolean(default=False)
# Epsilon parameter for the Adam optimizer. Ignored for other
# optimizers. Default: 1e-8
adam_epsilon = float(default=1e-8)
# Variable scope indicating which variables in the graph to train. Optional, if None
# all variables will be trained as normal.
variables_to_train = string(default=None)

[Prediction]
# Whether the data files contain the true classification
# values. Generally True for simulations and False for actual data.
# Optional when not running in predict mode.
true_labels_given = boolean(default=True)
# Whether to export predictions as a file when running in predict mode. Optional when not running in predict mode.
export_as_file = boolean(default=False)
# String filepath to file to save predictions (required when running in predict mode when ExportAsFile is
# True). Optional when not running in predict mode.
prediction_file_path = string(default=None)

[Logging]
# String filepath to directory to store TensorFlow model checkpoints and
# summaries. A timestamped copy of the configuration file will be made here.
model_directory = string(default='/tmp/ctalearn/')

[Debug]
# Whether to run TF debugger.
run_TFDBG = boolean(default=False)
