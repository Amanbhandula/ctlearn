[Data]
    # String identifying the format of the data files being loaded from. 
    # Determines the subclass of DataLoader which should be used when reading.
    format = HDF5
    # String path to a text file containing a list of data files (1 per line).
    file_list = /home/datasets/file_list/txt
    #file_list = /home/shevek/datasets/prototype/prototype_files_class_balanced.txt 
    # Boolean indicating whether or not to apply processing to loaded data.
    # Determines whether a DataProcessor is instantiated. If false,
    # settings under the Data Processing are not used.
    apply_processing = True
    
    # These options are used directly when instantiating a
    # DataLoader.
    [[Data Loading]]
        # Required string. Whether the examples to be loaded should be single telescope
        # (images from a single event on a single telescope). Note: this should agree with
        # the model type and other settings.
        example_type = array
        # List of strings identifying which telescope types to load.
        selected_tel_type = MSTS
        # Optional ist of telescope ids (integers) identifying which telescopes to load.
        # Telescope ids must be from the selected tel type. 
        # selected_tel_ids = 
        # An integer indicating the minimum
        # number of triggered images which must be 
        # present in each event for it to be used.
        min_num_tels = 1 
        # A PyTables cut condition used when selecting events. 
        # See https://www.pytables.org/usersguide/condition_syntax.html
        # for explanation of syntax. The names of valid event attributes
        # should be found in the HDF5 data files.
        cut_condition = ""
        # Fraction of the data to set aside for periodic validation. 
        validation_split = 0.1
        # Random seed used when splitting the data into training and
        # validation sets.
        #seed = 
              
    [[Data Processing]]
        # Whether or not to crop telescope images.
        crop = True
        # Type of image cleaning to apply when calculating the weighted
        # centroid position of each telescope image for cropping.
        image_cleaning = twolevel
        # Whether to return the cleaned images after cropping, or only use the
        # cleaning for determining the bounding box location and return the
        # original images.
        return_cleaned_images = False
        # Normalization method to be applied pixel-wise to each image. Log normalization
        # shifts the pixel values to [1,infinity) by subtracting the minimum pixel value
        # across all images, then takes the natural log.
        normalization = None
        # Whether to order the images within each telescope type before stacking them.
        # Trigger orders them so all triggered images are at the front with blank images/padding
        # behind. Size sorts images by the sum of pixel values, from largest to smallest.
        sort_images_by = trigger

        # The side length of the square region to crop telescope images
        # to. Set separately for each telescope type.
        [[[bounding_box_sizes]]]
            MSTS = 48
             
        # Cleaning thresholds used for image cleaning. For two-level
        # cleaning, the thresholds are a 2-tuple. 
        # The first value is the picture threshold, which passes all pixels
        # in the image greater than it, and the second is the boundary threshold,
        # which adds all pixels greater than it and adjacent to previously selected pixels.
        [[[thresholds]]]
            MSTS = 5.5, 1.0
            
   
    # Settings used by the TF Dataset when providing data.
    [[Data Input]]
        # Number of examples per batch for training/prediction.
        batch_size = 64
        # Whether to prefetch examples in the TF Dataset.
        prefetch = True
        # Max number of examples to prefetch and store in buffer when
        # loading data.
        prefetch_buffer_size = 10
        # Whether to apply Dataset.map on examples (must be true for HDF5 data).
        map = False
        # Number of parallel threads to use when loading examples with
        # Dataset.map
        num_parallel_calls = 1
        # Whether to shuffle examples.
        shuffle = True
        # Number of examples to shuffle at once. Larger buffer sizes
        # give a more uniform shuffle at the cost of a longer time to shuffle.
        shuffle_buffer_size = 10000

# Settings applied when mapping from trace to 2D images.
[Image Mapping]

[Model]
    # String filepath to directory containing models.
    model_directory = /home/models
    # Module in ModelDirectory containing function implementing
    # model. Included, verified modules are "single_tel", "cnn_rnn".
    model_module = cnn_rnn
    # Function in ModelModule implementing model. Included,
    # verified models are "single_tel_model" within the "single_tel" module and
    # "cnn_rnn_model" within the "cnn_rnn" module.
    model_function = cnn_rnn_model
    
    # Additional user-determined custom  model parameters
    [[Model Parameters]]
    # Single Tel must specify Network.
    # Network to be used for image classification. The only supported verified 
    # function is "basic_conv_block" within the "basic" module.
    NetworkModule = basic
    NetworkFunction = basic_conv_block

    # CNN-RNN and Variable Input Model must specify CNNBlock.
    # Network to be used for single telescope blocks. The only supported, verified
    # function is "basic_conv_block" within the "basic" module.
    CNNBlockModule = basic
    CNNBlockFunction = basic_conv_block

    # Variable Input Model must specify TelescopeCombination. Supported options are
    # "vector" and "feature_maps". Use "vector" for network heads expected input as
    # a single feature vector and "feature_maps" for those expecting input as a 
    # stack of feature maps as used for convolutional layers.
    TelescopeCombination = feature_maps

    # Variable Input Model must specify NetworkHead.
    # Network to be for array-level processing on the combined telescope outputs.
    # The supported functions are "basic_conv_head" and "basic_fc_head" within
    # the "basic" module.
    NetworkHeadModule = basic
    NetworkHeadFunction = basic_conv_head

    # Model configuration for Basic Conv Block model.
    # Pipe-separated string. Filters for each CNN Block convolutional layer.
    BasicConvBlockFilters = 32|64|128
    # Pipe-separated string. Kernels for each CNN Block convolutional layer.
    BasicConvBlockKernels = 3|3|3
    # Boolean. Whether to perform max pooling after each standard convolution.
    # Default: True
    BasicConvBlockMaxPool = True
    # Int. Max pool size. Required only if ...MaxPool is True.
    BasicConvBlockMaxPoolSize = 2
    # Int. Max pool strides. Required only if ...MaxPool is True.
    BasicConvBlockMaxPoolStrides = 2
    # Boolean. Whether to perform a final 1x1 convolution. Default: False
    BasicConvBlockBottleneck = False
    # Int. Number of output filters of final 1x1 convolution.
    # Required only if ...Bottleneck is True.
    BasicConvBlockBottleneckFilters = 8
    # Boolean. Whether to include a batch normalization layer after each
    # convolutional layer (including bottleneck). May lead to issues in
    # array-level models. Default: False
    BasicConvBlockBatchNorm = False

    # Model configuration for Basic Fully Connected Head model.
    # Pipe-separated string. Number of units for each FC Head dense layer.
    BasicFCHeadLayers = 1024|512|256|128|64
    # Boolean. Whether to include a batch normalization layer after each
    # dense layer. Default: False
    BasicFCHeadBatchNorm = False

    # Model configuration for Basic Convolutional Head model.
    # Pipe-separated string. Filters for each Conv Head convolutional layer.
    BasicConvHeadFilters = 64|128|256
    # Pipe-separated string. Kernels for each Conv Head convolutional layer.
    BasicConvHeadKernels = 3|3|3
    # Boolean. Whether to perform average pooling over spatial dimensions of 
    # convolutional output before calculating final output. Default: True
    BasicConvHeadAvgPool = True
    # Boolean. Whether to include a batch normalization layer after each
    # convolutional layer. # Default: False
    BasicConvHeadBatchNorm = False

    # Optional string. If specified, train on only variables within the scope
    # matching this name, freezing all others (useful when loading pretrained
    # weights). If not specified, train on all trainable variables.
    # Ex: with CNN-RNN, will freeze CNNBlock weights, training on NetworkHead only:
    # VariablesToTrain = NetworkHead
    VariablesToTrain =

    # The following options (all optional) are used by the provided models.

    # Path to a checkpoint file or model directory from which to
    # load pretrained CNNBlock weights. Default: "" (no weights loaded).
    # Used by: Single Tel, CNN-RNN
    PretrainedWeights = 
    # Float. Used in dropout layers. Default: 0.5
    # Used by: CNN-RNN
    DropoutRate = 0.5
    # Float. Decay parameter for Batch Norm layers. Default: 0.99
    # Used by: Basic (Single Tel)
    BatchNormDecay = 0.99

[Training]
# Number of epochs to run training and validation. If 0, run
# forever.
num_epochs = 0
# Number of training steps to run before evaluating on the 
# training and validation sets.
num_training_steps_per_validation = 1000

    [[Hyperparameters]]
        # Choice of optimizer function for training.
        optimizer = Adam
        # Base learning rate before scaling or annealing.
        base_learning_rate = 0.001
        # Whether to scale the learning rate by number of triggered telescopes (for trigger dropout).
        # Not used for single telescope models.
        scale_learning_rate = False
        # Whether to weight the loss to compensate for the class
        # balance.
        apply_class_weights = False
        # Epsilon parameter for the Adam optimizer. Ignored for other
        # optimizers. Default: 1e-8
        adam_epsilon = 1e-08
        # Variable scope indicating which variables in the graph to train. Optional, if None
        # all variables will be trained as normal.
        #variables_to_train =

[Prediction]
    # Whether the data files contain the true classification
    # values. Generally True for simulations and False for actual data.
    # Optional when not running in predict mode.
    true_labels_given = True
    # Whether to export predictions as a file when running in predict mode. Optional when not running in predict mode.
    export_as_file = False
    # String filepath to file to save predictions (required when running in predict mode when ExportAsFile is
    # True). Optional when not running in predict mode.
    prediction_file_path = None

[Logging]
    # String filepath to directory to store TensorFlow model checkpoints and
    # summaries. A timestamped copy of the configuration file will be made here.
    model_directory = /home/logs/example

[Debug]
    # Whether to run TF debugger.
    run_TFDBG = False
